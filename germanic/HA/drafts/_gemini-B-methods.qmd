# methods
## snc
16062.1.2.16063.1

please cf. @schwarz_this_2026 for the corpus building and evaluation script (still messy.)

## data

```{r,m1}
#| echo: false
#| output: false

mh<-lmdf.c$target=="0-human"
mp<-lmdf.c$target=="post"
mg<-lmdf.c$target=="gpt"

sh<-sum(lmdf.c$freq[c(which(mh))])
sp<-sum(lmdf.c$freq[c(which(mp))])
sg<-sum(lmdf.c$freq[c(which(mg))])
ts<-data.frame(target=c("gemini","human-pre","human-post"),tokens=c(sg,sh,sp))

```

our human langugae data consists of raw texts from german bundestag plenary protocols (@dip_dip_2026). the LLM corpus consists of  model summaries of a first subset of these texts generated with the following prompt: @sec-gemini-prompt.

### corpus subsets
`r knitr::kable(ts)`

### gemini prompt {#sec-gemini-prompt}
```{r,prompt-dep}
#| echo: false
#| warning: false
#| eval: false

ptx<-readLines("../g-prompt01.txt")
print(ptx)
```

```{r,prompt}
#| results: asis
#| echo: false
#| warning: false

ptx<-readLines("../g-prompt01.txt")
cat("```text\n",
    paste(ptx, collapse = "\n"),
    "\n```",
    sep = "")
```

## computation
we first devised AI-typical lemmata in the model corpus which are distinctive for that corpus using a linear regression model (R, package lme4::glmer(): @bates_fitting_2015) that calculates a score for each lemma in the corpus, see @fig-gpt.dplot and @fig-densplot01.

```{r, prelim}
#| echo: false
#| output: false


mh1<-lmdf.c$gus.c==1&lmdf.c$target=="0-human"
sh1<-sum(lmdf.c$freq[mh1])/sum(lmdf.c$freq[lmdf.c$target=="0-human"])
mp1<-lmdf.c$gus.c==1&lmdf.c$target=="post"
sp1<-sum(lmdf.c$freq[mp1])/sum(lmdf.c$freq[lmdf.c$target=="post"])
mg1<-lmdf.c$gus.c==1&lmdf.c$target=="gpt"
sg1<-sum(lmdf.c$freq[mg1])/sum(lmdf.c$freq[lmdf.c$target=="gpt"])
sp1>sh1 # TRUE!
### > more occurences (rel. frequencies) of gpt vocabular in post gpt corpus
sp1-sh1 # only 0.02587 points
p.df<-lmdf.c
p.df$target[p.df$target=="0-human"]<-"human"
s.df<-data.frame(target=c("human","post","DIFF:"),freq=c(sh1,sp1,sp1-sh1))
s.df$freq<-round(s.df$freq,4)

```


```{r,gptplot}
#| echo: false
#| output: true
#| label: fig-gpt.dplot
#| fig-cap: "lemma gpt scores over targets"

gpt.pplot<-plot(p.df$in.gp, p.df$f.rel, 
     col  = as.factor(p.df$target), 
     pch  = 19, 
     xlab = "in.gp (GPT typicality score)",
     ylab = "f.rel (relative frequency)",
     main = "GPT scores vs relative frequency by target"

     )
legend("topright", 
            legend = levels(as.factor(p.df$target)),
            col   = 1:length(levels(as.factor(p.df$target))), 
            pch   = 19)

```


```{r,gplot}
#| echo: false
#| output: true
#| label: fig-densplot01
#| fig-cap: "target gpt density"


library(ggplot2)
dff2<-p.df[c(which(mh1),which(mp1),which(mg1)),]
ggplot(data = dff2, aes(x = f.rel, fill = target)) +
  geom_density(alpha = 0.3) +
  coord_cartesian(xlim = c(0, 0.12)) +  # set your range here
  labs(title = "Density Plot", x = "GPT score", y = "Density") +
  theme_minimal()

```



# evaluation
## basic descriptive
to first gather an insight, yet with simple descriptive stats comparing the raw frequencies of gpt-preferred lemmas in pre- and post-gemini onset we find that in the target corpus the occurences of these lemma increase, only by small amount (see @tbl-s.df) and hard to visualise (see @fig-boxplot01). if these findings become relevant, we'll see in @sec-lm-1 where we evaluate the frequencies with a linear regression model.

```{r,kablediff}
#| echo: false
#| output: true
#| label: tbl-s.df
#| tbl-cap: "GPT lemma frequencies (table) over target. (freq / vH)"

knitr::kable(s.df,label = "tab-s.df")
```


```{r,boxdesc}
#| echo: false
#| output: true
#| label: fig-boxplot01
#| fig-cap: "GPT lemma frequencies (boxplot) over target. (freq / vH)"
#| warning: false


# boxplot(f.rel~target,p.df[c(which(mh1),which(mp1),which(mg1)),],outline=F,notch=T,
boxplot(f.rel~target,p.df[c(which(mh1),which(mp1)),],outline=F,notch=T,
main="descriptive stats: GPT vocab raw frequencies")

```

## responsible lemmata
selection of first 20 lemma that are responsible for the increase of frequency in general.

> this list is still in progress since bit complicated to sync descriptive lemma list with the linear model list derived, which looks more realistic for a gpt output but with lot of rubbish in it due to mislemmatization.

### lemma descriptive output
```{r,lemma-res-desc}
#| echo: false
#| output: true
#| warning: false

load("../lemma-resp.tj3.RData")

head(tj3$WORD[order(tj3$diff,decreasing = T)],20)
```

### lemma linear model output
```{r,lemma-res-lm}
#| echo: false
#| output: true
#| warning: false

load("../lemma-res.lres.RData")
lres.h<-head(lres,20)
lres.h$lemma
```


## linear regression {#sec-lm-1}
to prove descriptive results, we compute the stability of the frequency increase for target- vs. reference corpus with a linear regression model using R's lme4::lmer() function, cf. @bates_fitting_2015. coefficents are printed below, where frequency are the relative lemma frequencies over corpus; target defines reference resp. target corpus[post-gpt] (human/post) and in.gpt as numerical variable representing the gpt-score of the corresponding lemma i.e. wether it scores high (positive values) or low (negative values) in terms of being preferredly used by the chat agent.

```{r,lm-models}
#| echo: false
#| output: false
#| warning: false

library(lme4)
library(lmerTest)

lm1<-lm(f.rel~target*in.gp,lmdf.c)
lm2<-lmer(f.rel~target*in.gp+(1|lemma),lmdf.c)

```

### basic (lm)
formula: `frequency.relative ~ target * in.gpt`

```{r,lm}
#| echo: false
#| output: true
#| warning: false

summary(lm1)

```

### mixed effects model (lmer)
formula: `frequency.relative ~ target * in.gpt +(1|lemma)`

```{r,lmer}
#| echo: false
#| output: true
#| warning: false

s2<-summary(lm2)
s2
ingpvar<-s2$vcov@factors$correlation
ingpv<-ingpvar[,5]
ingpv<-ingpv[4]
#ingpv
pc8<-round(s2$coefficients[8,1],6)
pc1<-round(s2$coefficients[1,1],6)
#pc1<-s2$coefficients[1,1]
pcp<-round(pc8/pc1*100,2)
pc8<-round(s2$coefficients[8,1],6)
pc1<-round(s2$coefficients[1,1],6)
pcd<-pc1-pc8
pcd<-round(pcd,5)
#pcd
p100<-pc1*100
pcdp<-(pcd/p100)
#pcdp
pcdp<-round(pcdp,5)
pcd<-pc8
p100<-100/pc1
pcdp<-(pcd*p100)
#pcdp
pcdp<-round(pcdp,5)
#pcdp

#pcdp
#pcdp
#pc1<-s2$coefficients[1,1]
pcp<-round(pc8/pc1*100,2)

```

#### helper interpretation, to be tested
the coefficients interesting for us are the in.gp and targetpost:in.gp estimates. here we test the association between the gpt score of a lemma and its estimated frequency and its showing that a general increase of frequency is estimated if the score rises (=the lemma is within the lemmas preferred used by gemini) and that for the post-gpt corpus this increase  (``` `r pcdp`% ```) is significant (and not random in data).

in the fixed effects correlation output of the lmer() model we see that the gpt score correlates with the target corpus frequency for lemma by ``` `r ingpv` ```.

### anova of mixed effects model [out]
```{r,anova}
#| echo: false
#| output: false
#| warning: false

print(anova(lm1))
```

# references {#references}