
@misc{dip_dip_2026,
	address = {Berlin},
	type = {docs},
	title = {{DIP} - {Bundestagsprotokolle}},
	url = {https://dip.bundestag.de/%C3%BCber-dip/hilfe/api#content},
	urldate = {2026-01-24},
	journal = {DIP - API},
	author = {{DIP}},
	year = {2026},
}

@misc{ror_ror_2025,
	title = {{ROR} {Data}},
	url = {https://zenodo.org/records/17953395},
	doi = {10.5281/zenodo.17953395},
	abstract = {Data dump from the Research Organization Registry (ROR), a community-led registry of open identifiers for research organizations.

Release v2.0 contains ROR IDs and metadata for 120,445 research organizations in JSON and CSV format, in schema version 2. This includes the addition of 249 new records and metadata updates to 228 existing records. See the release notes.

Data format

Beginning with release v2.0 on 16 December 2025, data releases contain JSON and CSV files formatted only for schema v2. In addition, v2 files will no longer have `\_schema\_v2` appended to the end of the filename, ex v2.0-2025-12-16-ror-data.json. 

The CSV file contains a subset of fields from the JSON file, some of which have been flattened for easier parsing. As ROR records and the ROR schema are maintained in JSON, the CSV is for convenience only. JSON is the format of record.

Release versioning

Beginning with data release 2.0, files in the ROR Data dump on Zenodo will no longer include CSV and JSON files of the registry in the v1 schema. The ROR API default version is v2 as of July 2025, and v1 of the ROR API and dataset has been deprecated entirely, as of 8 December 2025. The data dump major version has been incremented to 2 per below.

Data releases are versioned as follows:



Minor versions (ex 1.1, 1.2, 1.3):  Contain changes to data, such as a new records and updates to existing records. No changes to the data model/structure.

Patch versions (ex 1.0.1):  Used infrequently to correct errors in a release. No changes to the data model/structure.

Major versions (ex 1.x, 2.x, 3.x):  Contains changes to data model/structure, as well as the data itself. Major versions will be released with significant advance notice.


For convenience, the date is also included in the release file name, ex: v2.0-2025-12-16-ror-data.zip.

The ROR data dump is provided under the  Creative Commons CC0 Public Domain Dedication. Location data in ROR comes from GeoNames and is licensed under a Creative Commons Attribution 4.0 license.},
	urldate = {2026-01-07},
	publisher = {Zenodo},
	author = {ROR, Research Organization Registry},
	month = dec,
	year = {2025},
	keywords = {PID, ROR, affiliation, institution identifier, open infrastructure, organization identifier, persistent identifier},
}

@misc{kraus_studie_2025,
	title = {Studie zeigt: {KI} lernt {Sprachregeln} beim {Lesen}},
	shorttitle = {Studie zeigt},
	url = {https://www.fau.de/2025/11/news/forschung/studie-ki-lernt-sprachregeln-beim-lesen/},
	abstract = {KI-Modelle sind in der Lage, Regeln der menschlichen Sprache herzuleiten, ohne dass sie mit expliziten Informationen über Grammatik und Wortklassen versorgt werden. Das haben Forscher der Friedrich…},
	language = {de},
	urldate = {2025-12-13},
	journal = {FAU},
	author = {Krauß, Patrick and {FAU U}},
	month = nov,
	year = {2025},
}

@misc{bsi_wie_2025,
	title = {Wie {KI} unsere {Sprache} verändert – {Eine} empirische {Studie}},
	url = {https://www.bsi.ag/cases/104-case-studie-wie-ki-unsere-sprache-veraendert---eine-empirische-studie.html},
	abstract = {Eine Studie zeigt, wie KI die Sprachkultur, Kommunikation und Interaktion mit Marken transformiert – und neue, hybride Sprachformen entstehen lässt.},
	language = {de},
	urldate = {2025-12-13},
	author = {BSI, Brand Science Institute},
	year = {2025},
}

@misc{ramirez_chatgpt_2025,
	title = {{ChatGPT} {Is} {Changing} the {Words} {We} {Use} in {Conversation}},
	url = {https://www.scientificamerican.com/article/chatgpt-is-changing-the-words-we-use-in-conversation/},
	abstract = {Words frequently used by ChatGPT, including “delve” and “meticulous,” are getting more common in spoken language, according to an analysis of more than 700,000 hours of videos and podcasts},
	language = {en},
	urldate = {2025-12-15},
	journal = {Scientific American},
	author = {Ramirez, Vanessa Bates},
	year = {2025},
}

@misc{flach_collostructions_2021,
	title = {Collostructions: {An} {R} implementation for the family of collostructional methods. {Package} version v.0.2.0.},
	url = {https://github.com/skeptikantin},
	language = {en},
	urldate = {2026-01-25},
	journal = {GitHub - skeptikantin},
	author = {Flach, Susanne},
	year = {2021},
}

@misc{wikipedia_google_2026,
	title = {Google {Gemini}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://de.wikipedia.org/w/index.php?title=Google_Gemini&oldid=263426206},
	abstract = {Google Gemini (ehemals Google Bard) ist ein von Google entwickelter KI-basierter, multimodaler Chatbot. Er wurde als direkte Reaktion auf den Erfolg von ChatGPT entwickelt und im März 2023 in eingeschränkter Kapazität veröffentlicht, bevor er im Laufe des Sommers in weiteren Ländern verfügbar wurde. Google Gemini ist mittlerweile in über 40 Sprachen verfügbar.},
	urldate = {2026-01-25},
	journal = {Wikipedia},
	author = {{Wikipedia} and {Google}},
	month = jan,
	year = {2026},
	note = {Page Version ID: 263426206},
}

@misc{fobbe_r_2026,
	title = {[{R}] {Source} {Code} des '{Corpus} der {Plenarprotokolle} des {Deutschen} {Bundestages}' ({CPP}-{BT}-{Source})},
	url = {https://zenodo.org/records/18177197},
	doi = {10.5281/zenodo.18177197},
	urldate = {2026-01-24},
	publisher = {Zenodo},
	author = {Fobbe, Sean},
	month = jan,
	year = {2026},
	note = {Language: deu},
	keywords = {Bundesrepublik Deutschland, Bundestag, Debatte, Deutscher Bundestag, Deutschland, Gesetz, Gesetzgebung, Legislative, Parlament, Parliament, Plenarprotokoll, Politik, R, Source Code, Verordnung},
}

@inproceedings{abrami_german_2022,
	address = {Marseille, France},
	title = {German {Parliamentary} {Corpus} ({GerParCor})},
	url = {https://aclanthology.org/2022.lrec-1.202/},
	abstract = {Parliamentary debates represent a large and partly unexploited treasure trove of publicly accessible texts. In the German-speaking area, there is a certain deficit of uniformly accessible and annotated corpora covering all German-speaking parliaments at the national and federal level. To address this gap, we introduce the German Parliamentary Corpus (GerParCor). GerParCor is a genre-specific corpus of (predominantly historical) German-language parliamentary protocols from three centuries and four countries, including state and federal level data. In addition, GerParCor contains conversions of scanned protocols and, in particular, of protocols in Fraktur converted via an OCR process based on Tesseract. All protocols were preprocessed by means of the NLP pipeline of spaCy3 and automatically annotated with metadata regarding their session date. GerParCor is made available in the XMI format of the UIMA project. In this way, GerParCor can be used as a large corpus of historical texts in the field of political communication for various tasks in NLP.},
	urldate = {2026-01-19},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Abrami, Giuseppe and Bagci, Mevlüt and Hammerla, Leon and Mehler, Alexander},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {1900--1906},
}

@misc{yakura_empirical_2025,
	title = {Empirical evidence of {Large} {Language} {Model}'s influence on human spoken communication},
	url = {http://arxiv.org/abs/2409.01754},
	doi = {10.48550/arXiv.2409.01754},
	abstract = {From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.},
	urldate = {2026-01-04},
	publisher = {arXiv},
	author = {Yakura, Hiromu and Lopez-Lopez, Ezequiel and Brinkmann, Levin and Serna, Ignacio and Gupta, Prateek and Soraperra, Ivan and Rahwan, Iyad},
	month = jul,
	year = {2025},
	note = {arXiv:2409.01754 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@inproceedings{shrivastava_repository-level_2023,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {Repository-level prompt generation for large language models of code},
	volume = {202},
	abstract = {With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex (Chen et al., 2021) used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a relative improvement of 36\% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at https://github.com/shrivastavadisha/repo\_level\_prompt\_generation.},
	urldate = {2026-01-03},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
	month = jul,
	year = {2023},
	pages = {31693--31715},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic register identification for the open web using multilingual deep learning},
	url = {https://arxiv.org/html/2406.19892v3},
	urldate = {2026-01-03},
}

@inproceedings{leidinger_language_2023,
	address = {Singapore},
	title = {The language of prompting: {What} linguistic properties make a prompt successful?},
	shorttitle = {The language of prompting},
	url = {https://aclanthology.org/2023.findings-emnlp.618/},
	doi = {10.18653/v1/2023.findings-emnlp.618},
	abstract = {The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with the task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on prompts which reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.},
	urldate = {2026-01-02},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Leidinger, Alina and van Rooij, Robert and Shutova, Ekaterina},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9210--9232},
}

@article{wu_corpus-based_2025,
	title = {A {Corpus}-{Based} {Multidimensional} {Analysis} of {Linguistic} {Features} between {Human}-{Authored} and {ChatGPT}-{Generated} {Compositions}},
	volume = {8},
	issn = {2617-0299},
	url = {https://al-kindipublishers.org/index.php/ijllt/article/view/9257},
	doi = {10.32996/ijllt.2025.8.5.10},
	language = {en},
	number = {5},
	urldate = {2026-01-02},
	journal = {International Journal of Linguistics, Literature and Translation},
	author = {Wu, JinLiang},
	month = may,
	year = {2025},
	keywords = {ChatGPT-Generated Compositions, Corpus-Based Analysis, Educational Proficiency Levels, Human-Authored Compositions, Second Language Writing},
	pages = {102--110},
}

@misc{duke_upress_critical_2025,
	title = {Critical {AI}},
	url = {https://www.dukeupress.edu/critical-ai},
	language = {en},
	urldate = {2025-12-08},
	author = {{Duke UPress}},
	year = {2025},
}

@article{aleksic_opinion_2025,
	title = {Opinion {\textbar} {It}’s happening: {People} are starting to talk like {ChatGPT}},
	issn = {0190-8286},
	shorttitle = {Opinion {\textbar} {It}’s happening},
	url = {https://www.washingtonpost.com/opinions/2025/08/20/chatgpt-claude-chatbots-language/},
	abstract = {Unnervingly, words overrepresented in chatbot responses are turning up more in human conversation.},
	language = {en-US},
	urldate = {2025-12-15},
	journal = {The Washington Post},
	author = {Aleksic, Adam},
	month = aug,
	year = {2025},
}

@misc{vechta_u_beyond_2024,
	title = {Beyond {Prompting}},
	url = {https://www.uni-vechta.de/beyondprompting/news/details/projekt-beyond-prompting-erfolgreiche-drittmitteleinwerbung},
	urldate = {2025-12-13},
	author = {{Vechta U}},
	year = {2024},
}

@article{roussel_einsatz_2024,
	title = {Einsatz von künstlicher {Intelligenz} im Überarbeitungsprozess von {Texten} in der {Fremdsprache} {Deutsch}: {Fokus} auf {Wortschatz} und {Syntax}},
	volume = {2024},
	shorttitle = {Einsatz von künstlicher {Intelligenz} im Überarbeitungsprozess von {Texten} in der {Fremdsprache} {Deutsch}},
	url = {https://hal.science/hal-04999057},
	number = {3},
	urldate = {2025-12-13},
	journal = {German as a Foreign Language},
	publisher = {GFL},
	author = {Roussel, Stephanie and Herdam, Ayaal},
	year = {2024},
	pages = {61--86},
}

@article{kalwa_noch_2025,
	title = {»{Noch} steckt {KI} in den {Kinderschuhen}«},
	volume = {55},
	issn = {2365-953X},
	url = {https://doi.org/10.1007/s41244-025-00379-0},
	doi = {10.1007/s41244-025-00379-0},
	abstract = {Der Beitrag untersucht Zeitreferenzen in Pressetexten, die die Bedeutung von sogenannter Künstlicher Intelligenz für den Menschen verhandeln, und interpretiert diese als Praktiken, die einerseits KI in spezifischer Weise konzeptualisieren und andererseits eine bestimmte Vorstellung von Zeitlichkeit hervorbringen. In diesem Zusammenhang führt der Beitrag die Methode der qualitativen Korpuslinguistik vor und zeigt auf, dass korpuslinguistische Verfahren auch für qualitative Analysen verwendet werden können.},
	language = {de},
	number = {2},
	urldate = {2025-12-13},
	journal = {Zeitschrift für Literaturwissenschaft und Linguistik},
	author = {Kalwa, Nina},
	month = jun,
	year = {2025},
	keywords = {Artificial Intelligence, Künstliche Intelligenz, Practices, Praktiken, Qualitative Corpus Linguistics, Qualitative Korpuslinguistik, Temporality, Temporalität, Time Reference, Zeitreferenz},
	pages = {379--405},
}

@article{milicka_ai_2025,
	title = {{AI} {Brown} and {AI} {Koditex}: {LLM}-{Generated} {Corpora} {Comparable} to {Traditional} {Corpora} of {English} and {Czech} {Texts}},
	shorttitle = {{AI} {Brown} and {AI} {Koditex}},
	url = {https://arxiv.org/abs/2509.22996},
	doi = {10.48550/arxiv.2509.22996},
	abstract = {This article presents two corpora of English and Czech texts generated with large language models (LLMs). The motivation is to create a resource for comparing human-written texts with LLM-generated text linguistically. Emphasis was placed on ensuring these resources are multi-genre and rich in terms of topics, authors, and text types, while maintaining comparability with existing human-created corpora. These generated corpora replicate reference human corpora: BE21 by Paul Baker, which is a modern version of the original Brown Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in Czech. The new corpora were generated using models from OpenAI, Anthropic, Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and are tagged according to the Universal Dependencies standard (i.e., they are tokenized, lemmatized, and morphologically and syntactically annotated). The subcorpus size varies according to the model used (the English part contains on average 864k tokens per model, 27M tokens altogether, the Czech partcontains on average 768k tokens per model, 21.5M tokens altogether). The corpora are freely available for download under the CC BY 4.0 license (the annotated data are under CC BY-NC-SA 4.0 licence) and are also accessible through the search interface of the Czech National Corpus.},
	language = {eng},
	urldate = {2025-11-24},
	author = {Milička, Jiří and Marklová, Anna and Cvrček, Václav},
	year = {2025},
	keywords = {ai},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} lme4},
	volume = {67},
	url = {https://github.com/lme4/lme4/},
	doi = {10.18637/jss.v067.i01},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	pages = {1--48},
}

