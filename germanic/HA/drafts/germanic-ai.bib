
@article{milicka_ai_2025,
	title = {{AI} {Brown} and {AI} {Koditex}: {LLM}-{Generated} {Corpora} {Comparable} to {Traditional} {Corpora} of {English} and {Czech} {Texts}},
	shorttitle = {{AI} {Brown} and {AI} {Koditex}},
	url = {https://arxiv.org/abs/2509.22996},
	doi = {10.48550/arxiv.2509.22996},
	abstract = {This article presents two corpora of English and Czech texts generated with large language models (LLMs). The motivation is to create a resource for comparing human-written texts with LLM-generated text linguistically. Emphasis was placed on ensuring these resources are multi-genre and rich in terms of topics, authors, and text types, while maintaining comparability with existing human-created corpora. These generated corpora replicate reference human corpora: BE21 by Paul Baker, which is a modern version of the original Brown Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in Czech. The new corpora were generated using models from OpenAI, Anthropic, Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and are tagged according to the Universal Dependencies standard (i.e., they are tokenized, lemmatized, and morphologically and syntactically annotated). The subcorpus size varies according to the model used (the English part contains on average 864k tokens per model, 27M tokens altogether, the Czech partcontains on average 768k tokens per model, 21.5M tokens altogether). The corpora are freely available for download under the CC BY 4.0 license (the annotated data are under CC BY-NC-SA 4.0 licence) and are also accessible through the search interface of the Czech National Corpus.},
	language = {eng},
	urldate = {2025-11-24},
	author = {Milička, Jiří and Marklová, Anna and Cvrček, Václav},
	year = {2025},
	keywords = {ai},
}

@misc{duke_upress_critical_2025,
	title = {Critical {AI}},
	url = {https://www.dukeupress.edu/critical-ai},
	language = {en},
	urldate = {2025-12-08},
	author = {{Duke UPress}},
	year = {2025},
	file = {Snapshot:/Users/guhl/Zotero/storage/5HSTYF8Y/critical-ai.html:text/html},
}

@misc{bsi_wie_2025,
	title = {Wie {KI} unsere {Sprache} verändert – {Eine} empirische {Studie}},
	url = {https://www.bsi.ag/cases/104-case-studie-wie-ki-unsere-sprache-veraendert---eine-empirische-studie.html},
	abstract = {Eine Studie zeigt, wie KI die Sprachkultur, Kommunikation und Interaktion mit Marken transformiert – und neue, hybride Sprachformen entstehen lässt.},
	language = {de},
	urldate = {2025-12-13},
	author = {BSI, Brand Science Institute},
	year = {2025},
	file = {Snapshot:/Users/guhl/Zotero/storage/32EG6S7M/104-case-studie-wie-ki-unsere-sprache-veraendert---eine-empirische-studie.html:text/html},
}

@misc{kraus_studie_2025,
	title = {Studie zeigt: {KI} lernt {Sprachregeln} beim {Lesen}},
	shorttitle = {Studie zeigt},
	url = {https://www.fau.de/2025/11/news/forschung/studie-ki-lernt-sprachregeln-beim-lesen/},
	abstract = {KI-Modelle sind in der Lage, Regeln der menschlichen Sprache herzuleiten, ohne dass sie mit expliziten Informationen über Grammatik und Wortklassen versorgt werden. Das haben Forscher der Friedrich…},
	language = {de},
	urldate = {2025-12-13},
	journal = {FAU},
	author = {Krauß, Patrick and FAU U},
	month = nov,
	year = {2025},
	file = {Snapshot:/Users/guhl/Zotero/storage/TT24RQXX/studie-ki-lernt-sprachregeln-beim-lesen.html:text/html},
}

@article{kalwa_noch_2025,
	title = {»{Noch} steckt {KI} in den {Kinderschuhen}«},
	volume = {55},
	issn = {2365-953X},
	url = {https://doi.org/10.1007/s41244-025-00379-0},
	doi = {10.1007/s41244-025-00379-0},
	abstract = {Der Beitrag untersucht Zeitreferenzen in Pressetexten, die die Bedeutung von sogenannter Künstlicher Intelligenz für den Menschen verhandeln, und interpretiert diese als Praktiken, die einerseits KI in spezifischer Weise konzeptualisieren und andererseits eine bestimmte Vorstellung von Zeitlichkeit hervorbringen. In diesem Zusammenhang führt der Beitrag die Methode der qualitativen Korpuslinguistik vor und zeigt auf, dass korpuslinguistische Verfahren auch für qualitative Analysen verwendet werden können.},
	language = {de},
	number = {2},
	urldate = {2025-12-13},
	journal = {Zeitschrift für Literaturwissenschaft und Linguistik},
	author = {Kalwa, Nina},
	month = jun,
	year = {2025},
	keywords = {Temporality, Künstliche Intelligenz, Artificial Intelligence, Practices, Praktiken, Qualitative Corpus Linguistics, Qualitative Korpuslinguistik, Temporalität, Time Reference, Zeitreferenz},
	pages = {379--405},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/2WWB9RAM/Kalwa - 2025 - »Noch steckt KI in den Kinderschuhen«.pdf:application/pdf},
}

@article{roussel_einsatz_2024,
	title = {Einsatz von künstlicher {Intelligenz} im Überarbeitungsprozess von {Texten} in der {Fremdsprache} {Deutsch}: {Fokus} auf {Wortschatz} und {Syntax}},
	volume = {2024},
	shorttitle = {Einsatz von künstlicher {Intelligenz} im Überarbeitungsprozess von {Texten} in der {Fremdsprache} {Deutsch}},
	url = {https://hal.science/hal-04999057},
	number = {3},
	urldate = {2025-12-13},
	journal = {German as a Foreign Language},
	author = {Roussel, Stephanie and Herdam, Ayaal},
	year = {2024},
	note = {Publisher: GFL},
	pages = {61--86},
	file = {HAL PDF Full Text:/Users/guhl/Zotero/storage/QSWMKGWA/Roussel und Herdam - 2024 - Einsatz von künstlicher Intelligenz im Überarbeitungsprozess von Texten in der Fremdsprache Deutsch.pdf:application/pdf},
}

@misc{vechta_u_beyond_2024,
	title = {Beyond {Prompting}},
	url = {https://www.uni-vechta.de/beyondprompting/news/details/projekt-beyond-prompting-erfolgreiche-drittmitteleinwerbung},
	urldate = {2025-12-13},
	author = {{Vechta U}},
	year = {2024},
	annote = {want to display as Vechta U, but isnt
},
	file = {Details - Uni Vechta:/Users/guhl/Zotero/storage/ABVMK3ED/projekt-beyond-prompting-erfolgreiche-drittmitteleinwerbung.html:text/html},
}

@misc{ramirez_chatgpt_2025,
	title = {{ChatGPT} {Is} {Changing} the {Words} {We} {Use} in {Conversation}},
	url = {https://www.scientificamerican.com/article/chatgpt-is-changing-the-words-we-use-in-conversation/},
	abstract = {Words frequently used by ChatGPT, including “delve” and “meticulous,” are getting more common in spoken language, according to an analysis of more than 700,000 hours of videos and podcasts},
	language = {en},
	urldate = {2025-12-15},
	journal = {Scientific American},
	author = {Ramirez, Vanessa Bates},
	year = {2025},
	file = {Snapshot:/Users/guhl/Zotero/storage/9Y8FEHBP/chatgpt-is-changing-the-words-we-use-in-conversation.html:text/html},
}

@article{aleksic_opinion_2025,
	title = {Opinion {\textbar} {It}’s happening: {People} are starting to talk like {ChatGPT}},
	issn = {0190-8286},
	shorttitle = {Opinion {\textbar} {It}’s happening},
	url = {https://www.washingtonpost.com/opinions/2025/08/20/chatgpt-claude-chatbots-language/},
	abstract = {Unnervingly, words overrepresented in chatbot responses are turning up more in human conversation.},
	language = {en-US},
	urldate = {2025-12-15},
	journal = {The Washington Post},
	author = {Aleksic, Adam},
	month = aug,
	year = {2025},
	file = {Snapshot:/Users/guhl/Zotero/storage/BRXLI35V/chatgpt-claude-chatbots-language.html:text/html},
}

@article{wu_corpus-based_2025,
	title = {A {Corpus}-{Based} {Multidimensional} {Analysis} of {Linguistic} {Features} between {Human}-{Authored} and {ChatGPT}-{Generated} {Compositions}},
	volume = {8},
	issn = {2617-0299},
	url = {https://al-kindipublishers.org/index.php/ijllt/article/view/9257},
	doi = {10.32996/ijllt.2025.8.5.10},
	language = {en},
	number = {5},
	urldate = {2026-01-02},
	journal = {International Journal of Linguistics, Literature and Translation},
	author = {Wu, JinLiang},
	month = may,
	year = {2025},
	keywords = {ChatGPT-Generated Compositions, Corpus-Based Analysis, Educational Proficiency Levels, Human-Authored Compositions, Second Language Writing},
	pages = {102--110},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/Z3ZXZFN7/Wu - 2025 - A Corpus-Based Multidimensional Analysis of Linguistic Features between Human-Authored and ChatGPT-G.pdf:application/pdf},
}

@inproceedings{leidinger_language_2023,
	address = {Singapore},
	title = {The language of prompting: {What} linguistic properties make a prompt successful?},
	shorttitle = {The language of prompting},
	url = {https://aclanthology.org/2023.findings-emnlp.618/},
	doi = {10.18653/v1/2023.findings-emnlp.618},
	abstract = {The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with the task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on prompts which reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.},
	urldate = {2026-01-02},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Leidinger, Alina and van Rooij, Robert and Shutova, Ekaterina},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9210--9232},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/SZTHQGNI/Leidinger et al. - 2023 - The language of prompting What linguistic properties make a prompt successful.pdf:application/pdf},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic register identification for the open web using multilingual deep learning},
	url = {https://arxiv.org/html/2406.19892v3},
	urldate = {2026-01-03},
	file = {Automatic register identification for the open web using multilingual deep learning:/Users/guhl/Zotero/storage/Q76WQ8WL/2406.html:text/html},
}

@inproceedings{shrivastava_repository-level_2023,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {Repository-level prompt generation for large language models of code},
	volume = {202},
	abstract = {With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex (Chen et al., 2021) used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a relative improvement of 36\% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at https://github.com/shrivastavadisha/repo\_level\_prompt\_generation.},
	urldate = {2026-01-03},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
	month = jul,
	year = {2023},
	pages = {31693--31715},
}

@misc{yakura_empirical_2025,
	title = {Empirical evidence of {Large} {Language} {Model}'s influence on human spoken communication},
	url = {http://arxiv.org/abs/2409.01754},
	doi = {10.48550/arXiv.2409.01754},
	abstract = {From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.},
	urldate = {2026-01-04},
	publisher = {arXiv},
	author = {Yakura, Hiromu and Lopez-Lopez, Ezequiel and Brinkmann, Levin and Serna, Ignacio and Gupta, Prateek and Soraperra, Ivan and Rahwan, Iyad},
	month = jul,
	year = {2025},
	note = {arXiv:2409.01754 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/guhl/Zotero/storage/EQZUWJEJ/Yakura et al. - 2025 - Empirical evidence of Large Language Model's influence on human spoken communication.pdf:application/pdf;Snapshot:/Users/guhl/Zotero/storage/UV25KDKD/2409.html:text/html},
}
