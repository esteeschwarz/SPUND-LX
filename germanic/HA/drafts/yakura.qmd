---
title: yakura replique
bibliography: germanic-ai.bib
#nocite: '@pethes_war_2006'
# tables:
#   - tbl_qa: {file: "qa.qmd", num: 1}
#   - tbl-summary: {file: "results.qmd", num: 2}

---

{{< include _vars.qmd >}}

## notes on replication study of @yakura_empirical_2025
we started replicating the workflow presented in above study beginning with creating the target corpus. @yakura_empirical_2025 built corpora from transcribed youtube video audios and podcasts, referencing a research organisations registry (@registry_ror_2025) to collect videos from institutional educational youtube channels. we assemlbled data the same way restricted to german institutions. @yakura_empirical_2025 limited the target corpus to 4 years before the introduction of the GPT chat agent on 2022-11-31 up to 2024-05-31. this allowed a timebased analysis of hypothesised appearance of AI-speech induced variances.

we tested the workflow until state of video transcribed using the youtube search API for finding the corresponding video channels, llama3.2 to match the correct channel within the search results, yt-dlp to download the video audio, ffmpeg library to convert to PCM .wav and whisper AI to finally transcribe the audio. all worked well ([script](https://github.com/esteeschwarz/SPUND-LX/tree/main/germanic/HA/LLM-001.R)) with resulting two texts from Mannheim University channel youtube contributions. we estimate an overall server runtime of about 10h to download and transcribe to text all audio of above categorized channels.
