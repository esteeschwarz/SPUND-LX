
```{r bibmy, eval=F, echo=FALSE, warning=FALSE,message=FALSE}
#fetch zotero .bib online
#share <- runif(1)
# response<-GET("https://api.zotero.org/groups/4713246/collections/9LNRRJQN/items/top?format=bibtex")
# bib<-httr::content(response,"text")
# y<-tempfile("ref",fileext = ".bib")
# writeLines(bib,y)
# t<-Sys.time()
# #tf<-format(t,"%a %b %d %Y (%H.%M)")
# tf<-format(t,"%Y%m%d(%H.%M)")
```


```{r eval=T, results=F,echo=FALSE, warning=FALSE,message=FALSE}

if (is_html_output()) {
  print("html output")
  output.rmd<-"html"
  dlmsg<-paste0('download pdf <a href="https://ada-sub.dh-index.org/school/papers/017/devoir-2-handout.pdf">here</a>.')
} else if (is_latex_output()) {
  print("pdf")
  output.rmd<-"pdf"
  dlmsg<-'view handout [online](https://ada-sub.dh-index.org/school/papers/017/).'
}
author<-"st. schwarz"
if(output.rmd=="html")
  author<-"st. schwarz"
```

```{r eval,echo=F,warning=F,message=FALSE,eval=FALSE}
options(scipen = 999)
wd<-paste0(Sys.getenv("GIT_TOP"),"/SPUND-LX/psych/HA/")
wd
df1<-read.csv(paste0(wd,"eval-001.csv"))
df1$m_rel<-df1$m/df1$corp_size
df1$m_rel<-round(df1$m_rel,5)
df1$ld<-round(df1$ld,3)

n_obs<-df1$corp_size[df1$corp=="obs"&df1$q=="a"]
n_ref<-df1$corp_size[df1$corp=="ref"&df1$q=="a"]
ld<-T
get.co<-function(data,ld=T){
# Center covariates
 t1<-data$range[data$q=="a"]
 t2<-diff(t1)
# t3<-
data$range_c    <- data$range    - mean(data$range[data$q=="a"]) # level intercept for conditions b-f 
data$range_c    <- data$range    - mean(data$range[data$q=="a"]) # level intercept for conditions b-f 
#data$corpsize_c <- data$corp_size - mean(data$corp_size)
#data$m_rel_c    <- data$m_rel    - mean(data$m_rel)
data$m_rel_c    <- data$m_rel - mean(data$m_rel[data$q=="a"])
t1<-c(1200,500)
t2<-abs(t1[1]-t1[2])-abs(t1[1]-t1[2])
# Corpus dummy
data$corpusB <- ifelse(data$corp == 'ref', 1, 0)
data$corpusA <- ifelse(data$corp == 'obs', 1, 0)
# data$ld<-round(ifelse(data$corp=="obs",df1$ld[df1$corp=="obs"],df1$ld[df1$corp=="ref"]),3)
# Dummy code condition (a-e) into 4 dummy vars (base = 'a')
data$cond_a <- ifelse(data$q == 'a', 1, 0)
data$cond_b <- ifelse(data$q == 'b', 1, 0)
data$cond_c <- ifelse(data$q == 'c', 1, 0)
data$cond_d <- ifelse(data$q == 'd', 1, 0)
data$cond_e <- ifelse(data$q == 'e', 1, 0)
data$cond_f <- ifelse(data$q == 'f', 1, 0)

# table(data$q)
# dummy_matrix <- as.matrix(cbind(
#   data$cond_b,
#   data$cond_c,
#   data$cond_d,
#   data$cond_e
# ))

# qr(dummy_matrix)$rank
# cor(data$corpusB, data$cond_b)
# cor(data$corpusB, data$cond_c)
# cor(data$corpusB, data$cond_d)
# cor(data$corpusB, data$cond_e)

ifelse(ld,X <- as.matrix(cbind(
  1,
  data$corpusA,
  data$range_c,
  #data$corpsize_c,
  data$m_rel_c,
  data$ld,
#  data$cond_a,
  data$cond_b,
  data$cond_c,
  data$cond_d,
  data$cond_e,
  data$cond_f
)),X <- as.matrix(cbind(
  1,
  data$corpusA,
  data$range_c,
  #data$corpsize_c,
  data$m_rel_c,
#  data$cond_a,
  data$cond_b,
  data$cond_c,
  data$cond_d,
  data$cond_e,
  data$cond_f
))
)

qr(X)$rank  # should equal ncol(X) = 8

Y <- data$dist

XtX <- t(X) %*% X
XtY <- t(X) %*% Y
beta_hat <- solve(XtX) %*% XtY
#?solve
# m<-matrix(c(1,0,2,1,1,1,1,1,3),ncol = 3)
# m
# solve(m)
# 2^-1
residuals <- Y - X %*% beta_hat
n <- nrow(X)
k <- ncol(X)
sigma2_hat <- sum(residuals^2) / (n - k)

cov_beta <- sigma2_hat * solve(XtX)
std_errors <- sqrt(diag(cov_beta))

t_value <- beta_hat[2] / std_errors[2]
df <- n - k
p_value <- 2 * pt(-abs(t_value), df)
# 1st: 0.352
# 2nd, wt intercept = query(0) : 0.07
coeff<-solve(t(X) %*% X) %*% t(X) %*% data$dist
coeff<-round(coeff,3)
#coeff
co.df<-data.frame(coeff,row.names = c("intercept",colnames(data)[c(12,3,7,13,14,15,16,17)]))
#co.df
return(list(p=p_value,coeff=co.df))
}
co.list<-get.co(df1,F)
co.df<-co.list$coeff
p_value<-round(co.list$p,5)
co.df
```

---

# xTitle
# proposition & coherence in :schizophrenia: threads
### stephan schwarz / a. stefanowitsch:16827_25S:sprache und psychose
## subject
Investigate reference marking, coherence and information structure in schizophrenia language by measuring distance of similar nouns within range of comment thread preceded by certain determinants.[^1]
## background
Inspired by Zimmerer et alii (#REF) we are interested in observations concerning coherence and propositional conditions in schizophrenia language, as these linguistic markers appear underinvestigated in research while they seem to play a crucial role within target group language. (As such seen as asset of thinking or world building capacity which might suffer from linguistic deficits within the range of positive symptoms.)
## method
To compute distances we queried a corpus for matching conditions where certain (assumed) determiners appear before similar nouns. This distance should give us information structural evidence of how strong these noun occurences are connected, i.e. if a noun appears out of the blue mostly or if it somewhere before has been introduced to the audience. In information structure definitions this would be termed with **given and new information** Prince (1981#REF).
----
## questions
Measuring the referent-reference distance which we here assume as indicator of coherence we hope to find empirical evidence for disturbed or not world building capabilities within schizophrenia language. Premising that a large noun distance indicates a low reference-referent association we hypothesise that in a language/TOM setting where the speakers estimation of the audiences context understanding capacities is disturbed we will find higer medium scores for the distance under matching conditions.
## daten
We built a corpus of the reddit r/schizophrenia thread (```n=`r n_obs` ``` tokens) and a reference corpus of r/unpopularopinion (```n=`r n_ref` ```). The corpus has been pos-tagged using the R udpipe:: package #REF which tags according to the universal dependencies tagset maintained by #REF. Still the `r n_obs` tokens can only, with the workflow of growing the corpus and devising the noun distances developed be just a starting point from where with more datapoints statistical evaluation becomes relevant first.   
The dataframe used for modeling consists of ``` `r length(dfa$q)` ``` distance datapoints derived from the postagged corpus.

```{r queries,warning=F,message=F,echo=F,eval=T}
# library(jsonlite)
# library(abind)
 #qs<-readLines(paste0(Sys.getenv("GIT_TOP"),"/SPUND-LX/psych/HA/eval-qs.json"))
 qjs<-fromJSON(paste0(Sys.getenv("GIT_TOP"),"/SPUND-LX/psych/HA/eval-qs.json"))
source(paste0(Sys.getenv("GIT_TOP"),"/SPUND-LX/psych/HA/eval-002.R"))
n_obs<-dfa$corpsize[dfa$target=="obs"][1]
n_ref<-dfa$corpsize[dfa$target=="ref"][1]
md_target<-median(dfa$dist[dfa$target=="obs"])
md_ref<-median(dfa$dist[dfa$target=="ref"])
#qsdf<-data.frame(matrix(unlist(qs),ncol = 6))
#qsdf<-abind(qs,along=0)

#qs <- stack(lapply(qs, function(x) x$token))

#colnames(qs) <- c("determiner", "q")

#print(qs)
#print("queries/ conditions")
#print(qs)
#cat(qs)
sample.df<-dfa[sample(1:length(dfa$dist),10),colnames(dfa)!="query_long"]
#sample.df$mf_rel<-round(sample.df$mf_rel,4)
#sample.df$ld<-round(sample.df$ld,4)
kable(sample.df)
```

----
## results
```{r,df1-kable,message=F,warning=F,echo=FALSE,eval=F}
#knitr::include_graphics("~/Documents/GitHub/school/papers/017/twitter-x.jpg",dpi = 150)
#kable(df1)
```

```{r,df1-print,message=F,warning=F,echo=FALSE,eval=F}
#knitr::include_graphics("~/Documents/GitHub/school/papers/017/twitter-x.jpg",dpi = 150)
#df1
```

```{r,df1-vis,message=F,warning=F,echo=FALSE,eval=T}
#knitr::include_graphics("~/Documents/GitHub/school/papers/017/twitter-x.jpg",dpi = 150)
# Reshape data for grouped barplot
# library(reshape2)
# df1<-dfa
# df1_no_outliers <- df1 %>%
#   group_by(target) %>%
#   filter(
#     dist >= quantile(dist, 0.25) - 1.5 * IQR(dist),
#     dist <= quantile(dist, 0.75) + 1.5 * IQR(dist)
#   ) %>%
#   ungroup()
# #boxplot(dist~target,df1_no_outliers)
# 
# df1_wide <- dcast(df1_no_outliers, q ~ target, value.var = "dist")
# df1_wide <- dcast(df1_no_outliers,q+target~dist,median)
# df1_wide <- dcast(dfa,q+target~dist,median)
# df1_wide <- acast(dfa,q+target~dist,median)
# 
# # Ensure the order of conditions a-f
# df1_wide$q <- factor(df1_wide$q, levels = c("a", "b", "c", "d", "e", "f"))
# df1_wide <- df1_wide[order(df1_wide$q), ]
# #rowSum(df1_wide)
# # Create the matrix for barplot
# #bar_matrix <- t(as.matrix(df1_wide[, c("obs", "ref")]))
# t.sum<-rowSums(df1_wide[,3:length(df1_wide)],na.rm = T)
# t.sum<-rowSums(df1_wide,na.rm = T)
#t.sum
#bar_matrix <- t.sum
# Plot grouped barplot
# barplot(
#   bar_matrix,
#   beside = TRUE,
# #  names.arg = c("0",levels(df1_wide$q)[2:6]),
#   col = c("black", "red"),
#   las = 1,
#   cex.names = 0.7,
#   main = "Distances distribution over conditions",
#   ylab = "same-noun distance"
# )
# legend("topright", legend = c("obs", "ref"), fill = c("black", "red"))
bar_mat <- tapply(dfe$median, list(dfe$q, dfe$target), identity)
bar_mat <- t(bar_mat)  # barplot expects groups in columns

# Make grouped barplot
df.plot<-barplot(bar_mat,
        beside = TRUE,
        col = c("black", "red"),
        names.arg = levels(dfe$q),
        legend.text = rownames(bar_mat),
        args.legend = list(x = "right"),
        ylab = "median distance",
        main = "distance distribution by query")


cat("## conditions:\n")
#print(unlist(qjs))
#qjs$b
#queries<-qjs
queries.df<-data.frame(q=letters[1:6],precedent=c("ALL (.*)",queries[2:6]),pos="NOUN")
kable(queries.df)
```

```{r lme, message=F,warning=F,echo=FALSE,eval=T}
# source(paste0(Sys.getenv("GIT_TOP"),"/SPUND-LX/psych/HA/eval-002.R"))
#print(head(df))
#print("anova_model <- aov(dist ~ target*q, data = df)")
#print(an.summ)
p.target<-anlm.summ["target",length(anlm.summ)]
p.target.q<-anlm.summ["target:q",length(anlm.summ)]
sig.array<-c(0,0.001,0.01,0.05,0.1,1)
p.sig<-p.target<sig.array
p.sig<-which(p.sig)[1]
p.sig.ex<-paste0("p<",sig.array[p.sig])
#kable(anlm.summ)
dist.target<-lm2.summ$coefficients["targetref",1]
dist.target<-round(dist.target,0)
#p.target.q
#get mean:
# m.target<-median(dfa$dist[dfa$target=="obs"])
# m.ref<-median(dfa$dist[dfa$target=="ref"])

#plot:
#unlist(qjs$b)
```

----
## conclusion
Over all conditions <!--**B** (``` `r unlist(qjs$b)` ```)-->we find significantly higher distance scores in the target corpus which proves our hypothesis. An ANOVA analysis of the linear regression model which posited a main effect of corpus\*q+range and random effects of lemma (`lme4::lmer(dist ~ corp*q+range + (1|lemma`) gets a p-value of ```p=`r p.target` ``` for the mean difference of ``` `r dist.target` ``` tokens (targetref) compared to the target.   
So the median distance of nouns, preceded by one of our queries, with ``` `r md_target` ``` tokens width for the target corpus and ``` `r md_ref` ``` in the reference corpus, is also with respect to the covariates significantly (```r p.sig.ex```) higher but still to be tested on a larger corpus.
## B. REF:
[^1]:snc.1:h2.pb.1000char/pg.queries