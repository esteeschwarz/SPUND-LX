{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571cd06-c492-46a9-9330-3f3a44f727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44239db0-24cd-4403-ab76-ec3f48d8a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import subprocess\n",
    "import sys\n",
    "import fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7aa97-0e99-49ea-a71a-f0da68c6f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacymodel = \"en_core_web_lg\"\n",
    "#import spacy\n",
    "spacymodel = \"en_core_web_lg\"\n",
    "# $ python -m spacy download en_core_web_lg\n",
    "\n",
    "#model = \"en_core_web_sm\"\n",
    "model = spacymodel\n",
    "try:\n",
    "    spacy.load(model)\n",
    "    print(f\"{model} already installed.\")\n",
    "except OSError:\n",
    "    print(f\"Downloading {model}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model])\n",
    "    nlp = spacy.load(model)\n",
    "    print(f\"{model} ready: {nlp.pipe_names}\")\n",
    "#print(sp.Default.stop_words)\n",
    "#nlp = spacy.load(model)\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "model = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68000517-bc66-467e-9f9d-73008b900ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cl = \"/SPUND/2025/hux/DAIS-C-Annotated - Upload/DAI-C-CL/Speaker Only_Raw\"\n",
    "path_co = \"/SPUND/2025/hux/DAIS-C-Annotated - Upload/DAI-C-CO/Speaker Only_Raw\"\n",
    "path_clinical = \"DAIS-C-Annotated - Upload/DAI-C-CL/Speaker Only_Raw\"\n",
    "path_control = \"DAIS-C-Annotated - Upload/DAI-C-CO/Speaker Only_Raw\"\n",
    "path_parquet = os.environ.get('HKW_TOP') + \"/SPUND/2025/hux/\"\n",
    "\n",
    "#path_clinical = os.environ.get('HKW_TOP') + path_cl\n",
    "#path_control = os.environ.get('HKW_TOP') + path_co\n",
    "print(os.environ.get('HKW_TOP'))\n",
    "path_clinical = os.environ.get('HKW_TOP') + path_cl\n",
    "path_control = os.environ.get('HKW_TOP') + path_co\n",
    "print(path_clinical)\n",
    "print(path_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56742cd8-8d1a-4f46-8892-e8da2983a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect file names and their content\n",
    "data = []\n",
    "for filename in os.listdir(path_clinical):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(path_clinical, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        data.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "# Convert to DataFrame\n",
    "clinical = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5330a7-9e21-4a0c-8a9a-a01854b4a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect file names and their content\n",
    "data = []\n",
    "for filename in os.listdir(path_control):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(path_control, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        data.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "# Convert to DataFrame\n",
    "control = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d30a6b-006d-4fd3-b085-6a6a8e96e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b87d3-2b02-4ed8-92c9-54a0d67c304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d12d4-38d1-44ce-9ad2-e7a8b2e62d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical[\"group\"] = \"scz\"\n",
    "control[\"group\"] = \"hc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6e14d-bd37-4b5a-9117-85958805a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1243a9a-8675-4f68-8f36-9d8e9626a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([clinical, control], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8338232-ef70-4690-8f57-45baba92c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\"\\n\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95202cd1-2bbb-45ad-ae58-95da59109360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom stopwords\n",
    "custom_stops = {\"er\", \"erm\", \"oh\", \"yeah\", \"mm\", \"mhm\", \"y\"}\n",
    "\n",
    "for w in custom_stops:\n",
    "    model.Defaults.stop_words.add(w)\n",
    "    model.vocab[w].is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547c788-33bb-4cf9-b41e-6f775bb196e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60781db-a24b-4a5d-9396-8529a19ab1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tokens(text):\n",
    "    spacy_text = model(text)\n",
    "    list_of_tokens = [\n",
    "        token.text\n",
    "        for token in spacy_text\n",
    "        if (not token.is_punct)\n",
    "        and (not token.is_stop)\n",
    "        and token.text.strip()              # removes whitespace-only tokens\n",
    "    ]\n",
    "    return list_of_tokens\n",
    "\n",
    "def get_chunks(list_of_tokens):\n",
    "    chunks = [list_of_tokens[i:i+10] for i in range(0, len(list_of_tokens), 10)]\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a51fb-60e3-4e35-80ce-df252d1e7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cleaned_tokens(\"hello world how are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ed94a-57a0-4d4c-992a-31ae6fdf1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"list_tokens\"] = df[\"text\"].apply(get_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b2e43-bdf7-45b2-b3f7-81f2b625edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5d431-7e1f-4b3f-b4bd-de836701699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"list_of_cunks\"] = df[\"list_tokens\"].apply(get_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3b0a2-8214-4c51-82a6-43d73c7c86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d20636-abe6-417f-bc2a-7c10b30a7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df.explode(\"list_of_cunks\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0746828-7b41-435a-a50d-3a324e15ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8189129-7c9e-4d95-82a1-24b58006cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.to_parquet(path_parquet+\"tabelle.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a080507-56a5-4d02-b99a-86856c4972e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae37b7-4b78-4e8c-81e0-1ce16c08eb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024969b9-f63d-4b85-af9a-b297134067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413e94f-ddc5-40e7-8485-884c9fba3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1861c5-f3e1-4913-8a45-bb63c50ae30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef7425-3b0a-435d-a628-dfe98a2b5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in text:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63550f87-f83d-40cd-9e00-8911cc250fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text.split()\n",
    "# wrong format list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcf838-3a9d-4b2b-8294-d11ca05d158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [sent.text for sent in model(text).sents]\n",
    "# wrong format list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf55b73-f9ae-4b8c-89d2-742b3576783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents\n",
    "text_l = df.iloc[1,3]\n",
    "print(text_l)\n",
    "import itertools\n",
    "#text = list(itertools.chain.from_iterable(text_l))\n",
    "#text = df_exploded[1,-1]\n",
    "text = ' '.join(text_l)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e9a15-a2b4-4f6b-b724-362bebb5711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "def build_nlp():\n",
    "#    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 1. Symptom onset expressions\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"SYMPTOM_ONSET\",\n",
    "        [\n",
    "            [{\"LOWER\": \"since\"}, {\"LOWER\": {\"IN\": [\"childhood\", \"infancy\", \"puberty\", \"teenage\", \"teens\"]}}],\n",
    "            [{\"LOWER\": \"since\"}, {\"LOWER\": \"i\"}, {\"LOWER\": \"was\"}, {\"IS_DIGIT\": True}],\n",
    "            [{\"LOWER\": \"back\"}, {\"LOWER\": \"in\"}, {\"LOWER\": {\"IN\": [\"college\", \"school\", \"university\", \"high\", \"uni\"]}}],\n",
    "            [{\"LOWER\": \"in\"}, {\"LOWER\": \"my\"}, {\"LOWER\": {\"IN\": [\"early\", \"mid\", \"late\"]}},\n",
    "             {\"LOWER\": {\"IN\": [\"teens\", \"twenties\", \"thirties\"]}}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 2. Episode-level markers\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"EPISODE_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": \"last\"}, {\"LOWER\": \"episode\"}, {\"LOWER\": \"in\"}, {\"ENT_TYPE\": \"DATE\"}],\n",
    "            [{\"LOWER\": \"during\"}, {\"LOWER\": \"my\"},\n",
    "             {\"LOWER\": {\"IN\": [\"worst\", \"last\", \"previous\"]}},\n",
    "             {\"LOWER\": {\"IN\": [\"period\", \"episode\"]}}, {\"LOWER\": \"in\"}, {\"ENT_TYPE\": \"DATE\"}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 3. Treatment-related\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"TREATMENT_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": {\"IN\": [\"started\", \"began\", \"quit\", \"stopped\"]}},\n",
    "             {\"LOWER\": {\"IN\": [\"meds\", \"medications\", \"therapy\"]}},\n",
    "             {\"LOWER\": \"in\"}, {\"ENT_TYPE\": \"DATE\"}],\n",
    "\n",
    "            [{\"LOWER\": {\"IN\": [\"hospitalised\", \"hospitalized\"]}},\n",
    "             {\"IS_DIGIT\": True}, {\"LOWER\": {\"IN\": [\"years\", \"months\", \"weeks\"]}},\n",
    "             {\"LOWER\": \"ago\"}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 4. Relapse\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"RELAPSE_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": {\"IN\": [\"relapsed\", \"relapse\"]}},\n",
    "             {\"LOWER\": \"again\"}, {\"LOWER\": {\"IN\": [\"this\", \"last\"]}},\n",
    "             {\"LOWER\": {\"IN\": [\"year\", \"month\", \"week\"]}}],\n",
    "\n",
    "            [{\"LOWER\": {\"IN\": [\"episode\", \"episodes\"]}},\n",
    "             {\"LOWER\": \"last\"}, {\"LOWER\": {\"IN\": [\"month\", \"year\"]}}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 5. DURATIONS (corrected)\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"DURATION_TEMPORAL\",\n",
    "        [\n",
    "            # For six months / for months / for a year\n",
    "            [\n",
    "                {\"LOWER\": \"for\"},\n",
    "                {\"LOWER\": {\"IN\": [\"a\", \"an\"]}, \"OP\": \"?\"},\n",
    "                {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"day\", \"days\", \"week\", \"weeks\", \"month\",\n",
    "                                   \"months\", \"year\", \"years\", \"decade\"]}}\n",
    "            ],\n",
    "\n",
    "            # past few months\n",
    "            [\n",
    "                {\"LOWER\": \"past\"},\n",
    "                {\"LOWER\": {\"IN\": [\"few\", \"couple\", \"several\"]}},\n",
    "                {\"LOWER\": {\"IN\": [\"days\", \"weeks\", \"months\", \"years\"]}}\n",
    "            ]\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 6. Life-event anchors\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"LIFE_EVENT_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": {\"IN\": [\"after\", \"before\", \"around\", \"during\"]}},\n",
    "             {\"LOWER\": {\"IN\": [\"the\", \"my\"]}, \"OP\": \"?\"},\n",
    "             {\"LOWER\": {\"IN\": [\n",
    "                 \"divorce\", \"breakup\", \"accident\", \"pregnancy\", \"diagnosis\",\n",
    "                 \"hospitalisation\", \"hospitalization\", \"finals\", \"lockdown\"\n",
    "             ]}}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 7. Calendar markers\n",
    "    # -------------------------------------\n",
    "    seasons = [\"spring\", \"summer\", \"autumn\", \"fall\", \"winter\"]\n",
    "    holidays = [\"christmas\", \"easter\", \"ramadan\", \"new\", \"new-year\", \"new year\"]\n",
    "\n",
    "    matcher.add(\n",
    "        \"CALENDAR_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": {\"IN\": [\"this\", \"last\"]}}, {\"LOWER\": {\"IN\": seasons}}],\n",
    "            [{\"LOWER\": {\"IN\": [\"this\", \"last\"]}}, {\"LOWER\": {\"IN\": holidays}}],\n",
    "            [{\"LOWER\": {\"IN\": [\"earlier\", \"end\"]}}, {\"LOWER\": \"this\"},\n",
    "             {\"LOWER\": {\"IN\": [\"week\", \"month\", \"year\"]}}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 8. Vague expressions\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"VAGUE_TEMPORAL\",\n",
    "        [\n",
    "            [{\"LOWER\": \"a\"}, {\"LOWER\": \"while\"}, {\"LOWER\": \"ago\"}],\n",
    "            [{\"LOWER\": \"ages\"}, {\"LOWER\": \"ago\"}],\n",
    "            [{\"LOWER\": \"the\"}, {\"LOWER\": \"other\"}, {\"LOWER\": \"day\"}],\n",
    "            [{\"LOWER\": \"recently\"}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 9. Numeric date formats\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"NUMERIC_DATE\",\n",
    "        [\n",
    "            # 12/2020, 05/21\n",
    "            [{\"SHAPE\": \"dd\"}, {\"TEXT\": \"/\"}, {\"SHAPE\": {\"IN\": [\"dd\", \"dddd\"]}}],\n",
    "            # 12.05.2020\n",
    "            [{\"SHAPE\": \"dd\"}, {\"TEXT\": \".\"}, {\"SHAPE\": \"dd\"},\n",
    "             {\"TEXT\": \".\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\", \"OP\": \"?\"}],\n",
    "            # 2020–21, 2019/2020\n",
    "            [{\"SHAPE\": \"dddd\"}, {\"TEXT\": {\"IN\": [\"–\", \"-\", \"/\"]}}, {\"SHAPE\": {\"IN\": [\"dd\", \"dddd\"]}}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 10. Diagnosis + date\n",
    "    # -------------------------------------\n",
    "    matcher.add(\n",
    "        \"DIAGNOSIS_DATE\",\n",
    "        [\n",
    "            [{\"LOWER\": {\"IN\": [\"diagnosed\", \"dx\"]}}, {\"LOWER\": \"in\"}, {\"ENT_TYPE\": \"DATE\"}],\n",
    "            [{\"LOWER\": {\"IN\": [\"diagnosed\", \"dx\"]}}, {\"IS_DIGIT\": True}],  # diagnosed at 14\n",
    "            [{\"LOWER\": {\"IN\": [\"got\", \"received\"]}}, {\"LOWER\": \"my\"},\n",
    "             {\"LOWER\": \"diagnosis\"}, {\"LOWER\": \"in\"}, {\"ENT_TYPE\": \"DATE\"}],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    nlp.matcher = matcher\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def extract_time_spans(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    spans = []\n",
    "\n",
    "    # built-in DATE entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DATE\":\n",
    "            spans.append(ent.text)\n",
    "\n",
    "    # rule-based matches\n",
    "    for match_id, start, end in nlp.matcher(doc):\n",
    "        spans.append(doc[start:end].text)\n",
    "\n",
    "    # dedupe and preserve order\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for s in spans:\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            final.append(s)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nlp = build_nlp()\n",
    "    \n",
    "    # text = \"\"\"\n",
    "    # I was diagnosed in 2018 after the accident.\n",
    "    # Symptoms have been there since childhood and got worse last winter.\n",
    "    # I relapsed again this year after quitting meds two months ago.\n",
    "    # Started therapy in June. Hospitalised three years ago.\n",
    "    # Had another episode last month and during lockdown things escalated.\n",
    "    # I struggled for six months before the diagnosis.\n",
    "    \n",
    "    # \"\"\"\n",
    "\n",
    "    print(extract_time_spans(text, nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98e769-caa0-4c53-b671-6dbb30f02dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
